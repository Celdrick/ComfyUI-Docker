# Run ComfyUI with ROCm on AMD GPU

*link:README.zh.adoc[中文说明]*

image:https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml/badge.svg["GitHub Workflow Status",link="https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml"]

https://hub.docker.com/r/yanwk/comfyui-boot/tags?name=rocm[View on <Docker Hub>]

## Prepare

* Make sure
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html[Radeon software for Linux with ROCm]
is installed on your Linux host.

## Build & Run

You may need to add these configuration (especially for APUs) into the command of `docker run`, `podman run` below. (Credit to
https://github.com/YanWenKun/ComfyUI-Docker/pull/67[nhtua])

* For RDNA 2 cards:
** `-e HSA_OVERRIDE_GFX_VERSION=10.3.0 \`

* For RDNA 3 cards:
** `-e HSA_OVERRIDE_GFX_VERSION=11.0.0 \`
** Check the https://rocm.docs.amd.com/en/latest/reference/gpu-arch-specs.html[AMD doc] to see if your GPU can use `11.0.1`.

* For RDNA 4 cards:
** `-e HSA_OVERRIDE_GFX_VERSION=12.0.0 \`
** Check the https://rocm.docs.amd.com/en/latest/reference/gpu-arch-specs.html[AMD doc] to see if your GPU can use `12.0.1`.

* For integrated graphics on CPU:
** `-e HIP_VISIBLE_DEVICES=0 \`

You may also want to add more environment variable(s):

* Enable tunable operations (slow first run, but faster subsequent runs.
https://github.com/ROCm/pytorch/tree/main/aten/src/ATen/cuda/tunable[Doc1],
https://github.com/Comfy-Org/docs/blob/main/troubleshooting/overview.mdx#amd-gpu-issues[Doc2]).
(Thanks to
https://github.com/YanWenKun/ComfyUI-Docker/pull/114[SergeyFilippov])

** `-e PYTORCH_TUNABLEOP_ENABLED=1 \`

.With Docker
[source,sh]
----
mkdir -p storage

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -e CLI_ARGS="" \
  yanwk/comfyui-boot:rocm
----

.With Podman
[source,sh]
----
mkdir -p storage

podman run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -e CLI_ARGS="" \
  yanwk/comfyui-boot:rocm
----

Once the app is loaded, visit http://localhost:8188/

[[hint]]
## ROCm: If you want to dive in...

__(Just side notes. Nothing to do with this Docker image)__

The commands below use the 
https://hub.docker.com/r/rocm/pytorch[AMD prebuilt ROCm PyTorch image].

This image is large in filesize. But if you have hard time to run the container, it may be helpful. As it takes care of PyTorch, the most important part, and you just need to install few more Python packages in order to run ComfyUI.

[source,sh]
----
docker pull rocm/pytorch:rocm6.4.2_ubuntu24.04_py3.12_pytorch_release_2.6.0

mkdir -p storage

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  --user root \
  --workdir /root/workdir \
  -v "$(pwd)"/storage:/root/workdir \
  rocm/pytorch:rocm6.4.2_ubuntu24.04_py3.12_pytorch_release_2.6.0 \
  /bin/bash

git clone https://github.com/comfyanonymous/ComfyUI.git

pip install -r ComfyUI/requirements.txt
# Or:
# conda install --yes --file ComfyUI/requirements.txt

python ComfyUI/main.py --listen --port 8188
# Or:
# python3 ComfyUI/main.py --listen --port 8188
----

## Additional notes for Windows users

__(Just side notes. Nothing to do with this Docker image)__

WSL2 supports ROCm and DirectML.

* ROCm

If your GPU is in the
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility/wsl/wsl_compatibility.html[Compatibility List],
you can either install
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/wsl/install-radeon.html[Radeon software]
in your WSL2 distro,
or use
<<hint, ROCm PyTorch image>>.

* DirectML

DirectML works for most GPUs (including AMD APU, Intel GPU).
It's slower than ROCm but still faster than CPU.
See: 
link:../docs/wsl-directml.adoc[Run ComfyUI on WSL2 with DirectML]. 

* ZLUDA

This is not using WSL2, it's running natively on Windows. ZLUDA can "translate" CUDA codes to run on AMD GPUs. But as the first step, I recommend to try running SD-WebUI with ZLUDA, it's easier to start with.
